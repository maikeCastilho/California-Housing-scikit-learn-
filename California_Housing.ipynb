{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEkDOHtXAPGB"
      },
      "outputs": [],
      "source": [
        "# ======================================================\n",
        "# Introdução\n",
        "# ======================================================\n",
        "# Este notebook foi preparado para ser executado no Google Colab.\n",
        "# Objetivo: treinar um modelo de Machine Learning (ex.: Rede Neural)\n",
        "# para prever o valor mediano de casas na Califórnia.\n",
        "#\n",
        "# Dataset: California Housing (scikit-learn)\n",
        "#\n",
        "# Fluxo:\n",
        "#   1. Carregar dataset\n",
        "#   2. Dividir em treino/teste\n",
        "#   3. Pré-processamento com ColumnTransformer\n",
        "#   4. Baseline com DummyRegressor\n",
        "#   5. Treinar seu modelo de ML (ex.: Rede Neural MLP)\n",
        "#   6. Avaliar métricas (MAE, RMSE, R²)\n",
        "#   7. Adicione PCA ao código e treine novamente o modelo de Rede Neural."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VfJua3QmAa3W"
      },
      "outputs": [],
      "source": [
        "# ======================================================\n",
        "# 1) Introdução\n",
        "# ======================================================\n",
        "# Prevendo o valor mediano de casas na Califórnia com Scikit-Learn.\n",
        "# Fluxo: baseline -> modelo MLP -> comparação com e sem PCA.\n",
        "\n",
        "import os\n",
        "import random\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "# Semente para reprodutibilidade\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UIrDHQWEAddn"
      },
      "outputs": [],
      "source": [
        "# ======================================================\n",
        "# 2) Funções Auxiliares\n",
        "# ======================================================\n",
        "\n",
        "def load_california_housing():\n",
        "    ds = fetch_california_housing(as_frame=True)\n",
        "    frame = ds.frame.copy()\n",
        "    y = frame[\"MedHouseVal\"]\n",
        "    X = frame.drop(columns=[\"MedHouseVal\"])\n",
        "    numeric_features = X.columns.tolist()\n",
        "    categorical_features = []  # não há colunas categóricas\n",
        "    return X, y, numeric_features, categorical_features\n",
        "\n",
        "\n",
        "def build_preprocessor(numeric_features, categorical_features):\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler())\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", numeric_transformer, numeric_features),\n",
        "        ]\n",
        "    )\n",
        "    return preprocessor\n",
        "\n",
        "\n",
        "def evaluate(model, X_test, y_test, label=\"Modelo\"):\n",
        "    preds = model.predict(X_test)\n",
        "    mae = mean_absolute_error(y_test, preds)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
        "    r2 = r2_score(y_test, preds)\n",
        "    print(f\"[{label}]  MAE={mae:.4f} | RMSE={rmse:.4f} | R²={r2:.4f}\")\n",
        "    return {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZxO1MJnAhPg",
        "outputId": "6914249d-588f-4b63-d137-2f8bcb1341a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset carregado. X shape=(20640, 8) | y shape=(20640,)\n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# 3) Carregar Dados e Split\n",
        "# ======================================================\n",
        "\n",
        "X, y, num_feats, cat_feats = load_california_housing()\n",
        "print(f\"Dataset carregado. X shape={X.shape} | y shape={y.shape}\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "preprocessor = build_preprocessor(num_feats, cat_feats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVnJrZd0Ajwx",
        "outputId": "90e0b457-f9e1-4951-da10-73bd5818acfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DummyRegressor (Baseline)]  MAE=0.9061 | RMSE=1.1449 | R²=-0.0002\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'MAE': 0.9060685490007149,\n",
              " 'RMSE': 1.1448563543099792,\n",
              " 'R2': -0.00021908714592466794}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ======================================================\n",
        "# 4) Baseline (DummyRegressor)\n",
        "# ======================================================\n",
        "\n",
        "baseline = Pipeline(steps=[\n",
        "    (\"pre\", preprocessor),\n",
        "    (\"reg\", DummyRegressor(strategy=\"mean\"))\n",
        "])\n",
        "\n",
        "baseline.fit(X_train, y_train)\n",
        "evaluate(baseline, X_test, y_test, label=\"DummyRegressor (Baseline)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0CZ_T6lAodW",
        "outputId": "032b7d21-fc96-4f6a-ab42-8d1feaab9b38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Rede Neural (MLP) sem PCA]  MAE=0.3675 | RMSE=0.5428 | R²=0.7752\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'MAE': 0.36752018639875844,\n",
              " 'RMSE': 0.54278293994409,\n",
              " 'R2': 0.7751745258264024}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ======================================================\n",
        "# 5) Modelo MLP sem PCA\n",
        "# ======================================================\n",
        "\n",
        "model_no_pca = Pipeline(steps=[\n",
        "    (\"pre\", preprocessor),\n",
        "    (\"reg\", MLPRegressor(\n",
        "        hidden_layer_sizes=(12, 6),\n",
        "        activation=\"relu\",\n",
        "        learning_rate_init=0.001,\n",
        "        max_iter=800,\n",
        "        random_state=RANDOM_SEED\n",
        "    ))\n",
        "])\n",
        "\n",
        "model_no_pca.fit(X_train, y_train)\n",
        "evaluate(model_no_pca, X_test, y_test, label=\"Rede Neural (MLP) sem PCA\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xO12pEdvAvUN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validação Cruzada R² (média ± desvio): 0.6853 ± 0.0035\n",
            "[Rede Neural (MLP) com PCA]  MAE=0.4609 | RMSE=0.6455 | R²=0.6821\n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# 6) Modelo MLP com PCA\n",
        "# ======================================================\n",
        "\n",
        "model_pca = Pipeline(steps=[\n",
        "    (\"pre\", preprocessor),\n",
        "    (\"pca\", PCA(n_components=0.95, random_state=RANDOM_SEED)),\n",
        "    (\"reg\", MLPRegressor(\n",
        "        hidden_layer_sizes=(12, 6),\n",
        "        activation=\"relu\",\n",
        "        learning_rate_init=0.001,\n",
        "        max_iter=800,\n",
        "        random_state=RANDOM_SEED\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Validação cruzada\n",
        "cv_scores = cross_val_score(model_pca, X_train, y_train, cv=5, scoring=\"r2\")\n",
        "print(f\"Validação Cruzada R² (média ± desvio): {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
        "\n",
        "# Treino final\n",
        "model_pca.fit(X_train, y_train)\n",
        "metrics = evaluate(model_pca, X_test, y_test, label=\"Rede Neural (MLP) com PCA\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Artefatos salvos em ./artifacts\n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# 7) Salvando Artefatos\n",
        "# ======================================================\n",
        "\n",
        "os.makedirs(\"artifacts\", exist_ok=True)\n",
        "\n",
        "# Salvar modelo treinado\n",
        "joblib.dump(model_pca, \"artifacts/model_california.joblib\")\n",
        "\n",
        "# Salvar métricas\n",
        "with open(\"artifacts/metrics.txt\", \"w\") as f:\n",
        "    for k, v in metrics.items():\n",
        "        f.write(f\"{k}: {v}\\n\")\n",
        "\n",
        "print(\"✅ Artefatos salvos em ./artifacts\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
